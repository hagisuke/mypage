＊複素NN
・論文
　https://arxiv.org/abs/1602.09046
　https://arxiv.org/abs/1705.09792
　https://arxiv.org/abs/1511.06351
・MRIに利用
　https://arxiv.org/abs/2004.01738
・実数CNNとの違い
　・入出力値も重みパラメータも複素数z=x+iy
　・パラメータ数は実部と虚部の分、2倍になる（ただし実部と虚部で制約関係にある）
　・分類問題なら損失はコサイン類似度でベクトルの近さを見る（正解も単位円上に変換）
　・表現力高い、過剰適合しづらい、性能は何とも

＊フーリエ
・MRI
　撮像：物理的プロセス（RFパルスでFID発生・勾配磁場印加）によるフーリエ変換でNMR信号取得
　　　　すなわち、k-spaceという実空間をフーリエ変換された空間上で計測される（実信号と虚信号があり複素数で表される）
　再構成：数学的プロセス（FFT）による多次元フーリエ逆変換で核磁気分布の2D/3D画像を得る（通常は信号強度Magnitudeを取って実数で表されるので、位相Phase情報が扱われない）
・CT
　投影：複数方向からX線を照射し、ラドン変換で投影データを得る
　再構成：複数方向からの投影データをそれぞれ1次元フーリエ変換して足し合わせ、2次元フーリエ逆変換して画像を得る

＊距離学習
・古典的アプローチ
　マハラノビス距離学習
・対照的(contrastive)アプローチ
　Contrastive loss、Triplet loss
・Softmaxをベースにしたアプローチ
　Center loss、SphereFace、CosFace、ArcFace
※Segmentationなら各ピクセルのFeatureをマッピングした空間で距離を扱う

＊MLP-Mixer
※従来CNNでは、特定の空間位置での特徴、異なる空間位置間での特徴、あるいはその両方を一度に混合する層で構成
・画像を9個のパッチに分割して入力
・パッチをチャンネル方向および空間方向に関してMLPで変換
・特定の位置ごとの操作と位置をまたいだ操作を明確に分ける

＊ConvMixer
※Res＝残差接続
・パッチサイズp(=3など)に分割するとする
・Patch Embeddingでh x (H/p) x (W/p)の特徴にする→GELU→BN
　カーネルサイズとストライドをpにしてInput次元c、Output次元hのConvを書けば良い
・ConvMixer Layer：Depthwise Conv＋GELU＋BN（Res付）→Pointwise Conv＋GELU＋BN
　ConvMixer Layerを繰り返す（通常より多きいカーネルサイズがDepthwise Convでは有効）
・最後はGlobal Average Pooling→Fully Connected
・実装
  nn.Conv2d(Cin=3,dim=h,k=p,s=p)
  nn.GELU()
  nn.BathNorm2d(dim)
  for i in range(depth):
    nn.Conv2d(dim,dim,k=p+2,groups=dim,padding='same'(=k//2))
    nn.GELU()
    nn.BathNorm2d(dim)
    x = F(x) + x # Residual
    nn.Conv2d(dim,dim,k=1)
    nn.GELU()
    nn.BathNorm2d(dim)
  nn.AdaptiveAvgPool2d((1,1))
  nn.Flatten()
  nn.Linear(dim,n_classes)
・肝
　パッチ分割、チャンネル方向と空間方向処理を明確に分ける、
　PoolやStrideによるダウンサンプリングが無い、解像度とサイズの表現を各層通じて維持
・計算量やメモリは必要だが精度出る
　Poolしてないので高周波成分が捨てられておらず、音声合成や生成系で復元可能

＊MetaFormer
・Patch分割してEmbedding
・Norm→Token Mixer（Res付）
　Token MixerはViTならAttention、MLP-MixerならSpatial MLP、Poolingでも良い
・Norm→Channel MLP（Res付）
・実装
  nn.GroupNorm(group=1, channel=dim)
  nn.AvgPool2d(pool_size=3,s=1,padding=pool_size//2,count_include_pad=False)
  x = F(x) + x
  nn.GroupNorm(group=1, channel=dim)
  nn.Conv2d(in_features, hidden_features, k=1)
  nn.GELU()
  nn.Conv2d(hidden_features, out_features, k=1)
  x = F(x) + x
・肝
　ViT系は過学習対策のData Augmentationが大事
　効率のため、Stride付Convによるステージ間の特徴マップ縮小でハイブリッドにするのが現実的
　MLPを1x1 convで表現している

＊GAN Metrics
・論文
　https://arxiv.org/pdf/1802.03446.pdf
　https://arxiv.org/pdf/2103.09396.pdf
・目視
・定性評価
　・NearestNeighbor
　・AMT
・定量評価
　・Inception Score、FID
・肝
　・Fidelity(image quality)とDiversity(variety)が重要
　・Pixel DistanceとFeature Distance
・条件付きの場合
　・On the Evaluation of Conditional GANs
　・一貫性も重要
　・FIDに条件を埋め込んだFJDを提案
・CycleGANの場合
　・Deep Snow: Synthesizing Remote Sensing Imagery with Generative Adversarial Nets
　FIDにcosine Resnet distanceを導入：ResNetにより真偽画像から各々抽出された特徴のコサイン類似度の平均
　RealとGeneratedサンプルの特徴量を3次元にPCAして学習進行による分布のマッチングを図示
・医療画像変換の場合のアイデア
　分布の近さ：
　　FIDのように特徴抽出する
　　NRDSのように真偽識別モデルをつくる
　　GQIのように分類性能を計算する
　　Perceptual Path Length：潜在空間が知覚的になめらか
　　RealとGeneratedの分布を描画し、その重なりからPrecisionとRecallを計算
　質：
　　画質：SSIM、PSNR、Sharpness Differene、平均パワースペクトル、ランダムフィルター応答の分布、コントラスト分布
　多様性：出力のエントロピー（出力と出力の平均の差の平均）
　　　　　※エントロピー（乱雑さ）はとがった分布になると小さくなる
　一貫性：外形や輪郭の一致