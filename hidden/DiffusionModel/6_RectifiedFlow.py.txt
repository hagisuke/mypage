import math
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm import tqdm
from scipy import integrate

tf.random.set_seed(42) # if you need to ensure reproducibility 

####################################
# STEP1: prepare datasets
####################################

img_size = 28
shuffle_buffer_size = 10000
batch_size = 128

def preprocess(image):
    # before batch
    image = tf.cast(image[..., tf.newaxis], tf.float32)  # (H, W, C)
    image = tf.clip_by_value(image / 255.0 * 2 - 1, -1.0, 1.0)  # â€“1 ~ 1
    return image

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
train_ds = tf.data.Dataset.from_tensor_slices(x_train)
train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)

####################################
# STEP2: prepare diffusion model
####################################

num_timesteps = 1000
time_eps = 1e-3

def time_encoding(timesteps, time_embed_dim, style='vaswani'):
    # you can also custom your own layer by extending the tf.keras.layers.Layer class
    if style=='milderhall':
        # positional encoding based on NeRF(Milderhall,2020)
        frequencies = tf.math.exp(tf.linspace(tf.math.log(1.), tf.math.log(1000.), time_embed_dim // 2))
        angular_speeds = 2.0 * tf.constant(math.pi) * frequencies
        time_vec = tf.concat([tf.math.sin(tf.expand_dims(tf.cast(timesteps, tf.float32) / tf.cast(num_timesteps, tf.float32), axis=-1) * tf.expand_dims(angular_speeds, axis=0)), 
                              tf.math.cos(tf.expand_dims(tf.cast(timesteps, tf.float32) / tf.cast(num_timesteps, tf.float32), axis=-1) * tf.expand_dims(angular_speeds, axis=0))], axis=-1)
    else:
        # positional encoding based on Transformer(Vaswani,2017)
        i = tf.cast(tf.range(0, time_embed_dim), tf.float32)
        div_term = tf.math.exp(i / time_embed_dim * tf.math.log(10000.)) # This style mean the same as `10000. ** (i / time_embed_dim)`, but is more efficient.
        time_vec = tf.concat([tf.math.sin(tf.expand_dims(tf.cast(timesteps, tf.float32), axis=-1) / tf.expand_dims(div_term[0::2], axis=0)), 
                              tf.math.cos(tf.expand_dims(tf.cast(timesteps, tf.float32), axis=-1) / tf.expand_dims(div_term[1::2], axis=0))], axis=-1)
    return time_vec

class ConvBlock(tf.keras.Model):
    def __init__(self, input_ch, output_ch, time_embed_dim):
        super(ConvBlock, self).__init__()
        self.convs = tf.keras.Sequential([
            tf.keras.layers.Conv2D(filters=output_ch, kernel_size=3, strides=1, padding='same'), # input_ch -> output_ch
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU(),
            tf.keras.layers.Conv2D(filters=output_ch, kernel_size=3, strides=1, padding='same'), # output_ch -> output_ch
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU()
        ])
        self.mlp = tf.keras.Sequential([
            tf.keras.layers.Dense(units=input_ch), # time_embed_dim -> input_ch
            tf.keras.layers.ReLU(),
            tf.keras.layers.Dense(units=input_ch) # input_ch -> input_ch
        ])

    def call(self, input_img, time_vec):
        time_h = self.mlp(time_vec)
        time_h = time_h[:,tf.newaxis,tf.newaxis,:]
        output_img = self.convs(input_img + time_h)
        return output_img

class UNet(tf.keras.Model):
    def __init__(self, input_ch=1, time_embed_dim=100):
        super(UNet, self).__init__()
        self.time_embed_dim = time_embed_dim

        self.down1 = ConvBlock(input_ch, 64, time_embed_dim)
        self.down2 = ConvBlock(64, 128, time_embed_dim)
        self.bot1 = ConvBlock(128, 256, time_embed_dim)
        self.up2 = ConvBlock(128 + 256, 128, time_embed_dim)
        self.up1 = ConvBlock(128 + 64, 64, time_embed_dim)
        self.out = tf.keras.layers.Conv2D(filters=input_ch, kernel_size=1, strides=1, padding='same')

        self.maxpool = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')
        self.upsample = tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear')
        self.concat = tf.keras.layers.Concatenate()

    def call(self, input_img, timesteps):
        time_vec = time_encoding(timesteps, self.time_embed_dim)

        h1 = self.down1(input_img, time_vec)
        h = self.maxpool(h1)
        h2 = self.down2(h, time_vec)
        h = self.maxpool(h2)

        h = self.bot1(h, time_vec)

        h = self.upsample(h)
        h = self.concat([h, h2])
        h = self.up2(h, time_vec)
        h = self.upsample(h)
        h = self.concat([h, h1])
        h = self.up1(h, time_vec)
        output_img = self.out(h)
        return output_img

class RectifiedFlow:
    def __init__(self, num_timesteps=1000, time_eps=1e-3):
        self.num_timesteps = num_timesteps
        self.time_eps = time_eps

    def add_noise(self, x_0, t):
        x0_shape = tf.shape(x_0)
        x_1 = tf.random.normal(x0_shape) # this time, x_1 is x_T: noise (0 and 1 are the opposite of the original paper)
        t_expand = tf.tile(t[:, tf.newaxis, tf.newaxis, tf.newaxis], [1, x0_shape[1], x0_shape[2], x0_shape[3]])
        x_t = t_expand * x_0 + (1. - t_expand) * x_1
        return x_t, x_1

    def reverse_to_img(self, x):
        x = tf.cast(tf.clip_by_value((x + 1) / 2 * 255, 0, 255), tf.uint8)
        return x.numpy()

    def sample_euler(self, model, diffusion_steps, x_shape=(20, 28, 28, 1)):
        batch_size = x_shape[0]
        x = tf.random.normal(x_shape)

        dt = 1. / diffusion_steps
        for i in tqdm(range(diffusion_steps)):
            t = tf.cast(tf.ones(batch_size) * ((i * dt * (1. - self.time_eps) + self.time_eps) * self.num_timesteps + 0.5), tf.int32)  # t = self.time_eps ~ 1. -> 1 ~ self.num_timesteps
            pred = model(x, t, training=False)
            x = x + pred * dt

        nfe = diffusion_steps
        images = [self.reverse_to_img(x[i]) for i in range(batch_size)]
        return images, nfe

    def sample_rungekutta(self, model, x_shape=(20, 28, 28, 1)):
        batch_size = x_shape[0]
        x = tf.random.normal(x_shape)
    
        def ode_func(i, x):
            t = tf.cast(tf.ones(batch_size) * (i * self.num_timesteps + 0.5), tf.int32)  # t = self.time_eps ~ 1. -> 1 ~ self.num_timesteps
            x = tf.reshape(tf.cast(x, tf.float32), x_shape)
            v = model(x, t, training=False)
            return v.numpy().reshape((-1,)).astype(np.float64)
        
        res = integrate.solve_ivp(ode_func, (self.time_eps, 1.), x.numpy().reshape((-1,)), method='RK45')
        nfe = res.nfev
        x = tf.reshape(tf.cast(res.y[:, -1], tf.float32), x_shape)
        images = [self.reverse_to_img(x[i]) for i in range(batch_size)]
        return images, nfe

model = UNet()
rectifier = RectifiedFlow(num_timesteps, time_eps)

####################################
# STEP3: training
####################################

lr = 1e-3
optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
train_loss = tf.keras.metrics.Mean(name='train_loss')

checkpoint_path = './checkpoints/train'
ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=None)
if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)

@tf.function
def train_step(x, t):
    with tf.GradientTape() as tape:
        x_noisy, noise = rectifier.add_noise(x, t)
        score_pred = model(x_noisy, tf.cast(t * num_timesteps + 0.5, tf.int32), training=True)  # t = time_eps ~ 1. -> 1 ~ num_timesteps
        loss = tf.keras.losses.MSE(x - noise, score_pred)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    train_loss.update_state(loss)

epochs = 10
for epoch in range(epochs):
    for images in tqdm(train_ds):
        t = tf.random.uniform([tf.shape(images)[0]], time_eps, 1., dtype=tf.float32)  # t = time_eps ~ 1.
        train_step(images, t)  # (bs, H, W, C), (bs,)

    print(f'Epoch {epoch + 1} | Loss: {train_loss.result().numpy()}')
    train_loss.reset_state()

    if (epoch + 1) % 10 == 0:
        ckpt_save_path = ckpt_manager.save()

####################################
# STEP4: sampling
####################################

def show_images(images, rows=2, cols=10):
    fig = plt.figure(figsize=(cols, rows))
    i = 0
    for r in range(rows):
        for c in range(cols):
            fig.add_subplot(rows, cols, i + 1)
            plt.imshow(images[i], cmap='gray')
            plt.axis('off')
            i += 1
    plt.show()

diffusion_steps = 10
images, nfe = rectifier.sample_euler(model, diffusion_steps)
show_images(images)
print(f'NFE: {nfe}')

images, nfe = rectifier.sample_rungekutta(model)
show_images(images)
print(f'NFE: {nfe}')
