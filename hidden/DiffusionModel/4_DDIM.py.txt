import math
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm import tqdm

tf.random.set_seed(42) # if you need to ensure reproducibility 

####################################
# STEP1: prepare datasets
####################################

img_size = 28
shuffle_buffer_size = 10000
batch_size = 128

def preprocess(image):
    image = tf.cast(image[..., tf.newaxis], tf.float32)  # (H, W, C)
    image = tf.clip_by_value(image / 255.0 * 2 - 1, -1.0, 1.0)  # â€“1 ~ 1
    return image

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
train_ds = tf.data.Dataset.from_tensor_slices(x_train)
train_ds = train_ds.shuffle(shuffle_buffer_size).batch(batch_size).map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)

####################################
# STEP2: prepare diffusion model
####################################

num_timesteps = 1000

def time_encoding(timesteps, time_embed_dim, style='vaswani'):
    # you can also custom your own layer by extending the tf.keras.layers.Layer class
    if style=='milderhall':
        # positional encoding based on NeRF(Milderhall,2020)
        frequencies = tf.math.exp(tf.linspace(tf.math.log(1.), tf.math.log(1000.), time_embed_dim // 2))
        angular_speeds = 2.0 * tf.constant(math.pi) * frequencies
        time_vec = tf.concat([tf.math.sin(tf.expand_dims(tf.cast(timesteps, tf.float32) / tf.cast(num_timesteps, tf.float32), axis=-1) * tf.expand_dims(angular_speeds, axis=0)), 
                              tf.math.cos(tf.expand_dims(tf.cast(timesteps, tf.float32) / tf.cast(num_timesteps, tf.float32), axis=-1) * tf.expand_dims(angular_speeds, axis=0))], axis=-1)
    else:
        # positional encoding based on Transformer(Vaswani,2017)
        i = tf.cast(tf.range(0, time_embed_dim), tf.float32)
        div_term = tf.math.exp(i / time_embed_dim * tf.math.log(10000.)) # This style mean the same as `10000. ** (i / time_embed_dim)`, but is more efficient.
        time_vec = tf.concat([tf.math.sin(tf.expand_dims(tf.cast(timesteps, tf.float32), axis=-1) / tf.expand_dims(div_term[0::2], axis=0)), 
                              tf.math.cos(tf.expand_dims(tf.cast(timesteps, tf.float32), axis=-1) / tf.expand_dims(div_term[1::2], axis=0))], axis=-1)
    return time_vec

class ConvBlock(tf.keras.Model):
    def __init__(self, input_ch, output_ch, time_embed_dim):
        super(ConvBlock, self).__init__()
        self.convs = tf.keras.Sequential([
            tf.keras.layers.Conv2D(filters=output_ch, kernel_size=3, strides=1, padding='same'), # input_ch -> output_ch
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU(),
            tf.keras.layers.Conv2D(filters=output_ch, kernel_size=3, strides=1, padding='same'), # output_ch -> output_ch
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU()
        ])
        self.mlp = tf.keras.Sequential([
            tf.keras.layers.Dense(units=input_ch), # time_embed_dim -> input_ch
            tf.keras.layers.ReLU(),
            tf.keras.layers.Dense(units=input_ch) # input_ch -> input_ch
        ])

    def call(self, input_img, time_vec):
        time_h = self.mlp(time_vec)
        time_h = time_h[:,tf.newaxis,tf.newaxis,:]
        output_img = self.convs(input_img + time_h)
        return output_img

class UNet(tf.keras.Model):
    def __init__(self, input_ch=1, time_embed_dim=100):
        super(UNet, self).__init__()
        self.time_embed_dim = time_embed_dim

        self.down1 = ConvBlock(input_ch, 64, time_embed_dim)
        self.down2 = ConvBlock(64, 128, time_embed_dim)
        self.bot1 = ConvBlock(128, 256, time_embed_dim)
        self.up2 = ConvBlock(128 + 256, 128, time_embed_dim)
        self.up1 = ConvBlock(128 + 64, 64, time_embed_dim)
        self.out = tf.keras.layers.Conv2D(filters=input_ch, kernel_size=1, strides=1, padding='same')

        self.maxpool = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')
        self.upsample = tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear')
        self.concat = tf.keras.layers.Concatenate()

    def call(self, input_img, timesteps):
        time_vec = time_encoding(timesteps, self.time_embed_dim)

        h1 = self.down1(input_img, time_vec)
        h = self.maxpool(h1)
        h2 = self.down2(h, time_vec)
        h = self.maxpool(h2)

        h = self.bot1(h, time_vec)

        h = self.upsample(h)
        h = self.concat([h, h2])
        h = self.up2(h, time_vec)
        h = self.upsample(h)
        h = self.concat([h, h1])
        h = self.up1(h, time_vec)
        output_img = self.out(h)
        return output_img

class Diffuser:
    def __init__(self, num_timesteps=1000, noise_schedule='linear'):
        self.num_timesteps = num_timesteps
        self.betas = tf.zeros(num_timesteps, tf.float32)
        self.alphas = tf.zeros(num_timesteps, tf.float32)
        self.alpha_bars = tf.zeros(num_timesteps, tf.float32)
        if noise_schedule=='cosine':
            self.cosine_noise_schedule()
        elif noise_schedule=='offsetcosine':
            self.offset_cosine_noise_schedule()
        else:
            self.linear_noise_schedule()

    def linear_noise_schedule(self):
        beta_start = 0.0001
        beta_end = 0.02
        self.betas = tf.linspace(beta_start, beta_end, self.num_timesteps)
        self.alphas = 1 - self.betas
        self.alpha_bars = tf.math.cumprod(self.alphas, axis=0)

    def cosine_noise_schedule(self):
        t = tf.cast(tf.range(0, self.num_timesteps + 1), tf.float32)
        f = tf.math.cos((t / self.num_timesteps) * tf.constant(math.pi) / 2) ** 2
        self.betas = 1.0 - f[1:] / f[:-1]
        self.alphas = 1 - self.betas
        self.alpha_bars = tf.math.cumprod(self.alphas, axis=0)

    def offset_cosine_noise_schedule(self):
        s = 0.008
        max_beta = 0.999
        t = tf.cast(tf.range(0, self.num_timesteps + 1), tf.float32)
        f = tf.math.cos((t / self.num_timesteps + s) / (1 + s) * tf.constant(math.pi) / 2) ** 2
        self.betas = tf.clip_by_value(1.0 - f[1:] / f[:-1], 0.0, max_beta)
        self.alphas = 1 - self.betas
        self.alpha_bars = tf.math.cumprod(self.alphas, axis=0)

    def add_noise(self, x_0, t):
        t_idx = t - 1  # t = 1 ~ self.num_timesteps -> t_idx = 0 ~ self.num_timesteps-1
        alpha_bar = tf.gather(self.alpha_bars, t_idx)  # (bs,)
        x0_shape = tf.shape(x_0)
        alpha_bar = tf.reshape(alpha_bar, [x0_shape[0]] + [1] * (len(x0_shape) - 1))  # (bs, 1, 1, 1)

        noise = tf.random.normal(x0_shape)
        x_t = tf.math.sqrt(alpha_bar) * x_0 + tf.math.sqrt(1 - alpha_bar) * noise
        return x_t, noise

    def denoise_onestep(self, model, x, t, step_size, islast=False):
        t_idx = t - 1  # t = 1 ~ self.num_timesteps -> t_idx = 0 ~ self.num_timesteps-1
        alpha = tf.gather(self.alphas, t_idx)
        alpha_bar = tf.gather(self.alpha_bars, t_idx)
        alpha_bar_prev = tf.gather(self.alpha_bars, tf.math.maximum(t_idx-step_size, 0))  # not use at t<=step_size

        x_shape = tf.shape(x)
        alpha = tf.reshape(alpha, [x_shape[0]] + [1] * (len(x_shape) - 1))
        alpha_bar = tf.reshape(alpha_bar, [x_shape[0]] + [1] * (len(x_shape) - 1))
        alpha_bar_prev = tf.reshape(alpha_bar_prev, [x_shape[0]] + [1] * (len(x_shape) - 1))

        eps = model(x, t, training=False)

        x0 = (x - tf.math.sqrt(1-alpha_bar) * eps) / tf.math.sqrt(alpha_bar)
        return x0 if islast else tf.math.sqrt(alpha_bar_prev) * x0 + tf.math.sqrt(1-alpha_bar_prev) * eps

    def reverse_to_img(self, x):
        x = tf.cast(tf.clip_by_value((x + 1) / 2 * 255, 0, 255), tf.uint8)
        return x.numpy()

    def sample(self, model, diffusion_steps, x_shape=(20, 28, 28, 1)):
        batch_size = x_shape[0]
        x = tf.random.normal(x_shape)

        step_size = self.num_timesteps // diffusion_steps
        for i in tqdm(range(self.num_timesteps, 0, -step_size)):
            t = tf.constant([i] * batch_size, dtype=tf.int32)
            islast = False
            if (i == step_size) or (i == self.num_timesteps % step_size):
                islast = True
            x = self.denoise_onestep(model, x, t, step_size, islast)

        images = [self.reverse_to_img(x[i]) for i in range(batch_size)]
        return images

model = UNet()
diffuser = Diffuser(num_timesteps)

####################################
# STEP3: training
####################################

lr = 1e-3
optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
train_loss = tf.keras.metrics.Mean(name='train_loss')

checkpoint_path = './checkpoints/train'
ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=None)
if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)

@tf.function
def train_step(x):
    with tf.GradientTape() as tape:
        t = tf.random.uniform([tf.shape(x)[0]], 1, num_timesteps + 1, dtype=tf.int32)  # (bs,)
        x_noisy, noise = diffuser.add_noise(x, t)
        noise_pred = model(x_noisy, t, training=True)
        loss = tf.keras.losses.MSE(noise, noise_pred)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    train_loss.update_state(loss)

epochs = 10
for epoch in range(epochs):
    for images in tqdm(train_ds):
        train_step(images)  # (bs, H, W, C)

    print(f'Epoch {epoch + 1} | Loss: {train_loss.result().numpy()}')
    train_loss.reset_state()

    if (epoch + 1) % 10 == 0:
        ckpt_save_path = ckpt_manager.save()

####################################
# STEP4: sampling
####################################

def show_images(images, rows=2, cols=10):
    fig = plt.figure(figsize=(cols, rows))
    i = 0
    for r in range(rows):
        for c in range(cols):
            fig.add_subplot(rows, cols, i + 1)
            plt.imshow(images[i], cmap='gray')
            plt.axis('off')
            i += 1
    plt.show()

diffusion_steps = 20
images = diffuser.sample(model, diffusion_steps)
show_images(images)